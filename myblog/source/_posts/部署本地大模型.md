---
title: 部署本地大模型
date: 2025-05-12 11:13:37
tags: llm
---
# 模型框架
>1. 企业级服务，SGLang 是不二之选：凭借卓越的性能，其吞吐量和结构化输出能力堪称行业翘楚，为企业级应用筑牢根基。<br>
https://docs.sglang.ai/start/install.html<br>
https://github.com/sgl-project/sglang<br>
>2. 在线高并发场景，VLLM 独占鳌头：凭借动态批处理和先进的内存管理技术，确保服务在高并发压力下依然稳定高效，保障业务流畅运行。<br>
https://docs.vllm.com.cn/en/latest/getting_started/installation/gpu.html<br>
https://github.com/vllm-project/vllm<br>
>3. 个人开发领域，Ollama 崭露头角：简单易用，跨平台支持搭配丰富的模型库，让创意灵感瞬间触手可及，助力个人开发者快速实现想法。<br>
https://github.com/ollama/ollama?tab=readme-ov-file

# LLM webui
>1. Dify：适合企业开发复杂 AI 应用，如智能客服、合同处理系统等，支持多模型协作和业务流程自动化。
https://dify.ai/zh<br>
https://github.com/langgenius/dify/blob/main/README_CN.md<br>
>2. Open-WebUI：适合个人开发者快速测试本地模型（如 Ollama 部署的 Llama3），或作为 ChatGPT 替代品进行日常交互。<br>
https://docs.openwebui.com/<br>
>3. Chatbox：面向非技术用户，提供无需代码的对话界面，支持快速体验多模型（如 GPT、Claude）的聊天能力。<br>
https://chatboxai.app/zh<br>
https://github.com/chatboxai/chatbox

# 部署
* 由于vllm和sglang需要资源较多，我们这里采用ollama + openwebui + deepseek
* 前提条件服务器已经配置了驱动和cuda nvidia-smi（驱动命令）nvcc（cuda命令）
* https://www.nvidia.cn/drivers/lookup/ 显卡下载run脚本运行
* https://developer.nvidia.com/cuda-toolkit-archive cuda下载

## 安装ollama
```shell
#https://github.com/ollama/ollama/tree/main/docs
#OLLAMA_MODELS 模型下载位置默认/usr/share/ollama/.ollama/models
#OLLAMA_HOST 监控地址默认127.0.0.1
curl -fsSL https://ollama.com/install.sh | sh
sed -i '/^Environment="PATH=/a Environment="OLLAMA_HOST=0.0.0.0"' /etc/systemd/system/ollama.service
systemctl daemon-reload
systemctl restart ollama.service
ollama run deepseek-r1
```
## 安装docker和nvidia-container-toolkit
```shell
#添加Docker软件包源
#添加Docker软件包源
sudo wget -O /etc/yum.repos.d/docker-ce.repo http://mirrors.cloud.aliyuncs.com/docker-ce/linux/centos/docker-ce.repo
sudo sed -i 's|https://mirrors.aliyun.com|http://mirrors.cloud.aliyuncs.com|g' /etc/yum.repos.d/docker-ce.repo
#安装Docker社区版本，容器运行时containerd.io，以及Docker构建和Compose插件
sudo yum -y install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
#启动Docker
sudo systemctl start docker
#设置Docker守护进程在系统启动时自动启动
sudo systemctl enable docker
#配置生产存储库
curl -s -L https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo | \
  sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo
#安装 NVIDIA Container Toolkit 软件包
sudo yum install -y nvidia-container-toolkit
#重启docker
sudo systemctl restart docker
```
## 安装webui
```shell
#可以通过-e OLLAMA_BASE_URL 配置ollama地址,进入web界面也可以配置,镜像差不多9G,在国外需要配置加速源
docker run -d -p 3000:8080 --gpus all -v open-webui:/app/backend/data --name open-webui ghcr.io/open-webui/open-webui:cuda
```

# 额外
* dify功能比Open-WebUI更强大，支持agent和工作流和很多插件，如果不想只单独通过webui来交互建议使用dify
```shell
curl -SL https://github.com/docker/compose/releases/download/v2.30.3/docker-compose-linux-x86_64 -o /usr/local/bin/docker-compose
#将可执行权限赋予安装目标路径中的独立二进制文件
sudo chmod +x /usr/local/bin/docker-compose
sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose
git clone https://github.com/langgenius/dify.git
cd dify
cd docker
cp .env.example .env
docker compose up -d
```
