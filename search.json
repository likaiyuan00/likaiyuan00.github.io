[{"title":"about","url":"/2025/04/14/about/","content":"分类\n","categories":["test"],"tags":["about"]},{"title":"elfk部署使用","url":"/2025/04/18/elfk/","content":"\nfilebeat不建议容器启动，适合放到每个节点采集日志统一发给logstash；如果全部输出到elasticsearch会导致负载比较高；不建议每个节点用logstash采集因为比较重，filebeat比较轻量级\n\n安装elfkcurl -SL https://github.com/docker/compose/releases/download/v2.30.3/docker-compose-linux-x86_64 -o /usr/local/bin/docker-compose#将可执行权限赋予安装目标路径中的独立二进制文件sudo chmod +x /usr/local/bin/docker-composesudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-composeyum install -y https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.3.2-x86_64.rpmcat &gt;&gt; ./elk.yml &lt;&lt; EOFversion: &#x27;3.8&#x27;services:  elasticsearch:    image: registry.cn-hangzhou.aliyuncs.com/lky-deploy/elasticsearch:7.14.0    container_name: elasticsearch    environment:      - discovery.type=single-node  # 单节点模式      - ES_JAVA_OPTS=-Xms512m -Xmx512m  # JVM 堆内存限制      - ELASTIC_PASSWORD=Ytest@123  # 设置 Elasticsearch 密码    volumes:      - ./elasticsearch/data:/usr/share/elasticsearch/data  # 数据持久化#      - ./elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml  # 自定义配置（可选）    ports:      - &quot;9200:9200&quot;  # REST API      - &quot;9300:9300&quot;  # 集群通信    networks:      - elk  logstash:    image: registry.cn-hangzhou.aliyuncs.com/lky-deploy/logstash:7.14.0    container_name: logstash    volumes:      - ./logstash/config/logstash.conf:/usr/share/logstash/pipeline/logstash.conf  # 自定义 Logstash 管道配置      - ./logstash/logs:/usr/share/logstash/logs  # 日志持久化    environment:      - LS_JAVA_OPTS=-Xms512m -Xmx512m  # JVM 堆内存限制    ports:      - &quot;5044:5044&quot;  # Beats 输入端口（如 Filebeat）      - &quot;5000:5000/tcp&quot;  # TCP 输入      - &quot;5000:5000/udp&quot;  # UDP 输入    depends_on:      - elasticsearch    networks:      - elk  kibana:    image: registry.cn-hangzhou.aliyuncs.com/lky-deploy/kibana:7.14.0    container_name: kibana    environment:      - I18N_LOCALE=zh-CN      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200  # 指向 Elasticsearch 服务      - ELASTICSEARCH_USERNAME=elastic  # 默认用户名      - ELASTICSEARCH_PASSWORD=Ytest@123  # 与 Elasticsearch 密码一致    ports:      - &quot;5601:5601&quot;  # Kibana Web 界面    depends_on:      - elasticsearch    networks:      - elknetworks:  elk:    driver: bridgeEOFmkdir ./logstash/config -pcat &gt;&gt; ./logstash/config/logstash.conf &lt;&lt; EOF# ./logstash/config/logstash.confinput &#123;  tcp &#123;    port =&gt; 5000  # 监听 TCP 日志  &#125;  beats &#123;    port =&gt; 5044  # 接收 Filebeat 输入  &#125;&#125;filter &#123;  grok &#123;    match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;COMBINEDAPACHELOG&#125;&quot; &#125;  # 解析 Apache 日志  &#125;  date &#123;    match =&gt; [ &quot;timestamp&quot;, &quot;dd/MMM/yyyy:HH:mm:ss Z&quot; ]  # 时间解析  &#125;&#125;output &#123;  elasticsearch &#123;    hosts =&gt; [&quot;elasticsearch:9200&quot;]    user =&gt; &quot;elastic&quot;    password =&gt; &quot;Ytest@123&quot;    index =&gt; &quot;logs-%&#123;+YYYY.MM.dd&#125;&quot;  # 按日期创建索引  &#125;&#125;EOFchmod 777 elasticsearch/data\nfilebeat根据不同tag写入不同的logstash后续分割和输出建立索引好区分\nfilebeat.inputs: # filebeat input输入- type: log    # 标准输入  enabled: true  # 启用标准输入  paths:    - /var/log/*  tags: [&quot;system&quot;]  #  fields:  #    type: &quot;system_log&quot;- type: filestream  paths:    - &quot;/var/log/nginx/*.log&quot;  tags: [&quot;nginx&quot;]   # 标记为 nginx 日志#output.console:# enabled: true               # 启用控制台输出  #  pretty: true                # 美化 JSON 格式  # codec.json:  #   pretty: true  # escape_html: false        # 不转义 HTML 符号（保持原始格式） # 输出到 Logstash - 用于生产数据处理output.logstash:  enabled: true               # 启用 Logstash 输出  #  when:  #    equals:  #      fields.type: &quot;system_log&quot;  hosts: [&quot;127.0.0.1:5044&quot;]  # Logstash 的地址和端口（支持多个主机负载均衡）  when.contains:      tags: &quot;system&quot;  # 匹配 tags 包含 &quot;system&quot;  hosts: [&quot;127.0.0.1:5045&quot;]  enabled: true  when.contains:    tags: &quot;nginx&quot;  # 匹配 tags 包含 &quot;nginx&quot;\nlogstash根据不同type进行过滤和输出索引\nLogstash Reference [7.10] | Elasticinput &#123;  tcp &#123;    port =&gt; 5000  # 监听 TCP 日志  &#125;  beats &#123;    port =&gt; 5044  # 接收 Filebeat 输入    type =&gt; &quot;system&quot;  &#125;  beats &#123;    port =&gt; 5045  # 接收 Filebeat 输入    type =&gt; &quot;nginx&quot;  &#125;&#125;   filter &#123;  date &#123;    match =&gt; [ &quot;timestamp&quot;, &quot;dd/MMM/yyyy:HH:mm:ss Z&quot; ]  # 时间解析  &#125;   if[type] == &quot;nginx&quot; &#123;    grok &#123;      match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;HTTPD_COMMONLOG&#125;&quot; &#125;  # 解析 nginx 日志,如果不区分；system类型是解析不了的，会直接报错      remove_field =&gt; [&quot;@version&quot;]     &#125;  &#125;  #对于system类型可以再写个if来单独过滤  if[type] == &quot;system&quot; &#123;    grok &#123;      match =&gt;  &#123;&quot;message&quot; =&gt; &quot;%&#123;IPV4:ip&#125;&quot;&#125;        remove_field =&gt; [&quot;@version&quot;]     &#125;    mutate &#123;  #这里过滤器乱写的，需要根据自身的业务配置        remove_field =&gt; [&quot;timestamp&quot;]        gsub =&gt; [&quot;message&quot;,&quot;\\s&quot;,&quot;| &quot;]        split =&gt; [&quot;message&quot;,&quot;|&quot;]        replace =&gt; &#123; &quot;timenew&quot; =&gt;  &quot;%&#123;+yyyy-MM-dd&#125;&quot; &#125;        add_field =&gt; &#123;         &quot;year&quot; =&gt; &quot;%&#123;+yyyy&#125;&quot;         &quot;month&quot; =&gt; &quot;%&#123;+MM&#125;&quot;         &quot;day&quot; =&gt; &quot;%&#123;+dd&#125;&quot;         &quot;status&quot; =&gt; &quot;%&#123;[message][1]&#125;&quot;         &quot;code&quot; =&gt; &quot;%&#123;[message][2]&#125;&quot;        &#125;    &#125;  &#125;   &#125;#必须通过type指定不同输出创建不同的index =&gt;,否则index的字段不一样，当第一个index结构确定后，第二个输入无法输出到第一个index，因为字段不一样output &#123;  if &quot;system&quot; in [tags] &#123;    elasticsearch &#123;      hosts =&gt; [&quot;elasticsearch:9200&quot;]      user =&gt; &quot;elastic&quot;      password =&gt; &quot;Ytest@123&quot;      index =&gt; &quot;filebeat-system-logs-%&#123;+YYYY.MM.dd&#125;&quot;  # 按日期创建索引    &#125;  &#125;    if &quot;nginx&quot; in [tags] &#123;    elasticsearch &#123;      hosts =&gt; [&quot;elasticsearch:9200&quot;]      user =&gt; &quot;elastic&quot;      password =&gt; &quot;Ytest@123&quot;      index =&gt; &quot;filebeat-nginx-logs-%&#123;+YYYY.MM.dd&#125;&quot;  # 按日期创建索引    &#125;  &#125;  &#125;\nelasticsearch\n常用语法\n\n&#x2F;_cat &#x2F;_cat&#x2F;master?help&#x2F;_cat&#x2F;indices?v  显示title&#x2F;_cat&#x2F;indiceslogs-2025.03.24 为索引名称&#x2F;logs-2025.03.24&#x2F;_search 查看文档&#x2F;logs-2025.03.24&#x2F; 查看索引结构&#x2F;logs-2025.03.24&#x2F;_doc&#x2F;_search?q&#x3D;message:test\n\n\n","categories":["中间件"]},{"title":"iptables防止ddos(cc)","url":"/2025/04/21/iptables%E9%98%B2%E6%AD%A2ddos-cc/","content":"\n基本上发行版都是自带的，轻量级，不需要额外下载Fail2Ban也可以但是需要额外下载\n\n如何配置使用iptables -I INPUT -p tcp --dport 80 -m state --state NEW -m recent --set参数    作用-I INPUT    将规则插入到 INPUT 链的最前面-p tcp --dport 80    匹配目标端口为 80 的 TCP 流量-m state --state NEW    仅匹配 新建连接（如 TCP 的 SYN 包）-m recent --set    将来源 IP 记录到 recent 模块的默认列表（/proc/net/xt_recent/DEFAULT）iptables -I INPUT -p tcp --dport 80 -m state --state NEW -m recent --update --seconds 60 --hitcount 100 -j DROP参数    作用-m recent --update --seconds 60 --hitcount 100    检查 IP 在 60 秒内是否发起超过 100 次新连接-j DROP    若超限，直接丢弃数据包\n\n效果图，到指定次数自动丢弃数据包，端口不通，到达指定时间自动恢复\n经过测试 –hitcount 大于20 会报错\n解决办法echo options xt_recent ip_pkt_list_tot=200 &gt; /etc/modprobe.d/xt.confmodprobe -r xt_recent &amp;&amp; modprobe xt_recent 重新加载查看 lsmod |grep xt  ；cat /sys/module/xt_recent/parameters/ip_pkt_list_tot 对应 xt.conf\n额外补充若其他规则也使用 recent 默认列表，可能导致误判，可以通过–name 指定名称分类\niptables -I INPUT -p tcp –dport 80 -m state –state NEW -m recent –set –name HTTP_CC\niptables -I INPUT -p tcp –dport 80 -m state –state NEW -m recent –update –seconds 60 –hitcount 200 –name HTTP_CC -j DROP\n则 &#x2F;proc&#x2F;net&#x2F;xt_recent&#x2F;HTTP_CC 叫 HTTP_CC\n","categories":["linux"]},{"title":"miniconda3","url":"/2025/04/21/miniconda3/","content":"\nconda是一个包和环境管理工具，用于创建、管理和切换Python的虚拟环境\n\n安装mkdir -p ~/miniconda3wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.shbash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3rm ~/miniconda3/miniconda.shsource ~/miniconda3/bin/activate\n使用1. conda --version #查看conda版本，验证是否安装2. conda update conda #更新至最新版本，也会更新其它相关包3. conda update --all #更新所有包4. conda update package_name #更新指定的包5. conda create -n env_name package_name #创建名为env_name的新环境，并在该环境下安装名为package_name 的包，可以指定新环境的版本号，例如：conda create -n python2 python=python2.7 numpy pandas，创建了python2环境，python版本为2.7，同时还安装了numpy pandas包6. source activate env_name #切换至env_name环境7. source deactivate #退出环境8. conda info -e #显示所有已经创建的环境9. conda create --name new_env_name --clone old_env_name #复制old_env_name为new_env_name10. conda remove --name env_name –all #删除环境11. conda list #查看所有已经安装的包12. conda install package_name #在当前环境中安装包13. conda install --name env_name package_name #在指定环境中安装包14. conda remove -- name env_name package #删除指定环境中的包15. conda remove package #删除当前环境中的包16. conda env remove -n env_name #采用第10条的方法删除环境失败时，可采用这种方法\n\n\n\n两个环境，一个有request一个没有，隔离作用\n镜像源# 查看镜像源conda config --show-sources# 添加镜像源conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main# 从镜像源中安装包时显示来源conda config --set show_channel_urls yes# 删除镜像源conda config --remove channels https://XXX# 删除配置的镜像源，使用默认镜像源conda config --remove-key channels\n\n打包运行环境pip install conda-packconda pack -n my_env_name -o out_name.tar.gztar -zxvf 2.7.tar.gz -C 2.7conda info -esource activate my_env_name\n\n","categories":["python"]},{"title":"openvpn","url":"/2025/04/21/openvpn/","content":"安装git clone https://github.com/likaiyuan00/openvpn-install.gitcd openvpn-install &amp;&amp; bash openvpn-install.sh#systemctl start openvpn@client.service 启动的账号密码  auth-user-pass 控制客户端密码验证echo &quot;test test@123&quot; &gt;  /etc/openvpn/userfile.sh\n\n配置文件字段解读server端在#openvpn服务端的监听地址local 0.0.0.0#openvpn服务端的监听端口（默认1194）port 1115#使用的协议，tcp/udpproto tcp#使用三层路由ip隧道（tun），还是二层以太网隧道（tap），一般使用tundev tun#ca证书、服务端证书、服务端秘钥和秘钥交换文件ca /etc/openvpn/server/ca.crtcert /etc/openvpn/server/server.crtkey /etc/openvpn/server/server.keydh /etc/openvpn/server/dh.pem#vpn服务端为自己和客户端分配的ip地址池。#服务端自己获取网段的第一个地址（此处是10.8.0.1），后为客户端分配其他的可用地址。以后客户端就可以和10.8.0.1进行通信。注意：以下网段地址不要和已有网段冲突或重复server 10.8.0.0  255.255.255.0#使用一个文件记录已分配虚拟ip的客户端和虚拟ip的对应关系。以后openvpn重启时，将可以按照此文件继续为对应的客户端分配此前相同的ip（自动续借ip）ifconfig-pool-persist ipp.txt#使用tap模式的时候考虑此选项server-bridge XXXXXX#vpn服务端向客户端推送vpn服务端内网网段的路由配置，以便让客户端能够找到服务端的内网。多条路由写多个push指令push &quot;route 10.0.10.0  255.255.255.0&quot;push &quot;route 192.168.10.0 255.255.255.0&quot;  #允许客户端访问的内网网段#让vpn客户端之间可以通信。默认情况客户端只能服务端进行通信#默认此项是注释的，客户端之间不能相互通信client-to-client#允许多个客户端使用同一个vpn账号连接服务端#默认是注释的，不支持多个客户端登录一个账号duplicate-cn#每10秒ping一次，120秒后没收到ping就说明对方挂了keepalive 10 120#加强认证方式，防攻击。如果配置文件中启用此项（默认是启用的），需要执行openvpn --genkey --secret ta.key，并把ta.key放到/etc/openvpn/server/目录，服务端第二个参数为0；同时客户端也要有此文件，且client.conf中此指令的第二个参数需要为1tls-auth /etc/openvpn/server/ta.key 0#选择一个密码。如果在服务器上使用了cipher选项，那么也必须在这里指定它。注意，v2.4客户端/服务端将在tls模式下自动协商AES-256-GCMcipher AES-256-CBC#openvpn 2.4版本的vpn才能设置此选项。表示服务端启用lz4的压缩功能 ，传输数据给客户端时会压缩数据包。Push后在客户端也配置启用lz4的压缩功能，向服务端发数据时也会压缩。如果是2.4版本以下的老版本，则使用用comp-lzo指令compress lz4-v2push &quot;compress lz4-v2&quot;#启用lzo数据压缩格式，此指令用于低于2.4版本的老版本，且如果服务端配置了该指令，客户端也必须要配置comp-lzo#并发客户端的连接数max-clients 1000#通过ping得知超时时，当重启vpn后将使用同一个秘钥文件以及保持tun连接状态persist-keypersist-tun#在文件中输出当前的连接信息，每分钟截断并重写一次该文件status openvpn-status.log#log指令表示每次启动vpn时覆盖式记录到指定日志文件中#log-append则表示每次启动vpn时追加式的记录到指定日志中#但两者只能选其一，或者不选时记录到rsyslog中log  /var/log/openvpn.loglog-append  /var/log/openvpn.log#日志记录的详细级别verb 3#当服务器重新启动时，通知客户端，以便它可以自动重新连接。仅在UDP协议是可用explicit-exit-notify 1#沉默的重复信息。最多20条相同消息类别的连续消息将输出到日志mute 20\nclient#标识这是个客户端client#使用的协议，tcp/udp，服务端是什么客户端就是什么proto tcp#使用三层路由ip隧道（tun），还是二层以太网隧道（tap），服务端是什么客户端就是什么dev tun#服务端的地址和端口remote 10.0.0.190 1194#一直尝试解析OpenVPN服务器的主机名resolv-retry infinite#大多数客户机不需要绑定到特定的本地端口号nobind#初始化后的降级特权(仅非windows)user nobodygroup nobody#尝试在重新启动时保留某些状态persist-keypersist-tun#ca证书、客户端证书、客户端密钥#如果它们和client.conf或client.ovpn在同一个目录下则可以不写绝对路径，否则需要写绝对路径调用ca ca.crtcert client.crtkey client.key#通过检查certicate是否具有正确的密钥使用设置来验证服务器证书。remote-cert-tls server#加强认证方式，防攻击。服务端有配置，则客户端必须有tls-auth ta.key 1#选择一个密码。如果在服务器上使用了cipher选项，那么您也必须在这里指定它。注意，v2.4客户端/服务器将在TLS模式下自动协商AES-256-GCM。cipher AES-256-CBC# 服务端用的什么，客户端就用的什么#表示客户端启用lz4的压缩功能，传输数据给客户端时会压缩数据包comp-lzo# 日志级别verb 3#沉默的重复信息。最多20条相同消息类别的连续消息将输出到日志mute 20\n\n如何直连openvpn服务端其他局域网服务器\n客户端（10.8.0.10） ping (服务端)172.16.1.7 正常 ping (服务端其他内网机器)172.16.1.8失败\n\n\n第一种方法 配置路由route add -net 10.8.0.0 netmask 255.255.255.0 gw 172.16.1.710.8.0.0  客户端IP172.16.1.7 openvpn 服务端IP\n\n\n\n\n\n\n\n第二种方法使用snat转发 iptables -t nat -A POSTROUTING -d 10.8.0.0&#x2F;24 -o eth0 -j MASQUERADEiptables -A FORWARD -s 10.8.0.0 -j ACCEPT\n\n\n\n额外服务端route 192.168.0.0 255.255.0.0   指令作用是在服务端加一条路由，网关是客户端ip\n服务端只能ping通客户端的tun0的ip，内网ip不行，即使加了路由也不行\n客户端push “route 192.168.10.0 255.255.255.0”作用是在客户端多加一条路由。网关是服务端的tun0IP（也就是server 指令配置分配的地址池）\n","categories":["linux"]},{"title":"prometheus","url":"/2025/04/18/prometheus/","content":"https://github.com/likaiyuan00/k8s-prometheus.git\nk8s-prometheus部署kubernetes_sd_configs配置文件只采集了\n\n1 prometheus*  prometheus-server2 container*   kubelet 的10250端口  &#x2F;metrics&#x2F;cadvisor3 node*    node_exporter4 apiserver*  apiserver 6443 端口 &#x2F;metrics5 kube*  kube-state-metrics组件 8080端口 &#x2F;metrics6 coredns*  kubernetes-pods 自动发现 pod需要配置 prometheus.io&#x2F;scrape: “true” 不然抓取不到 默认flaseprometheus.io&#x2F;path: “&#x2F;metrics”   # 指标路径（默认 &#x2F;metrics 可不写）7 kubelet*  apiserver代理端点 &#x2F;api&#x2F;v1&#x2F;nodes&#x2F;&lt;node-name&gt;&#x2F;proxy&#x2F;metrics其他有需要的可以自行配置\n\n导入镜像，执行yml文件即可\nprometheus效果图\ngrafana效果图\nkubelet 组件 kubelet 三个指标 &#x2F;metrics&#x2F;probes（探针） &#x2F;metrics&#x2F;cadvisor（pod） &#x2F;metrics（node）\n对应apiserver的 &#x2F;api&#x2F;v1&#x2F;nodes&#x2F;${node-name}&#x2F;proxy&#x2F;${url};一般为了减少apiserver的负载不建议使用这种方式 **\n直接访问会报401没有权限\n需要先获取token，上面文件执行完会有一个prometheus用户\npod内token路径为 &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount&#x2F;token\n通过token再去访问发现就正常了\n/metricscurl -k -sS  -H &quot;Authorization: Bearer $TOKEN&quot;  https://127.0.0.1:6443/api/v1/nodes/master/proxy/metricscurl -k -sS  -H &quot;Authorization: Bearer $TOKEN&quot;  https://127.0.0.1:10250/metrics\n\n对应kubelet*开头\n/metrics/probes（探针）curl -k -sS  -H &quot;Authorization: Bearer $TOKEN&quot;  https://127.0.0.1:6443/api/v1/nodes/master/proxy/metrics/probescurl -k -sS  -H &quot;Authorization: Bearer $TOKEN&quot;  https://127.0.0.1:10250/metrics/probes\n\n/metrics/cadvisor（pod）curl -k -sS  -H &quot;Authorization: Bearer $TOKEN&quot;  https://127.0.0.1:6443/api/v1/nodes/master/proxy/metrics/cadvisorcurl -k -sS  -H &quot;Authorization: Bearer $TOKEN&quot;  https://127.0.0.1:10250/metrics/cadvisor\n\n对应container*开头，容器指标\nnode_exporter端口暴露到节点了就不需要token了\nnode*开头，节点指标\nkube-state-metrics集群应用状态监控比较重要的一个需要单独安装使用containerPort: 8080 暴露到节点了不需要token\nkube*开头\napiserver主要是监控apiserver的qps,查询成功率失败率等信息\napiserver*开头\nkubernetes-pods 自动发现如果元数据内设置true，该pod才可以被抓取，默认false\n以coredns为例\n以coredns*开头\n这个自动发现还可以配置自身业务的监控，只有保证开启抓取，和符合prometheus抓取规范就可以，如果开启了prometheus.io&#x2F;scrape 但是pod并没有提供数据指标的能力就会直接报错，如图404\n比如现在我想加一个grafana的数据，只需要添加对应元数据就可以了\nprometheus就自动发现了pod的ip\ngrafana*开头\n","categories":["prometheus"],"tags":["prometheus"]},{"title":"screen","url":"/2025/04/27/screen/","content":"多终端管理神器ctrl +a + d 退出终端exit 退出加销毁终端\n常用参数$&gt; screen [-AmRvx -ls -wipe][-d &lt;作业名称&gt;][-h &lt;行数&gt;][-r &lt;作业名称&gt;][-s ][-S &lt;作业名称&gt;] -A 　将所有的视窗都调整为目前终端机的大小。-d   &lt;作业名称&gt; 　将指定的screen作业离线。-h   &lt;行数&gt; 　指定视窗的缓冲区行数。-m 　即使目前已在作业中的screen作业，仍强制建立新的screen作业。-r   &lt;作业名称&gt; 　恢复离线的screen作业。-R 　先试图恢复离线的作业。若找不到离线的作业，即建立新的screen作业。-s 　指定建立新视窗时，所要执行的shell。-S   &lt;作业名称&gt; 　指定screen作业的名称。-v 　显示版本信息。-x 　恢复之前离线的screen作业。-ls或--list 　显示目前所有的screen作业。-wipe 　检查目前所有的screen作业，并删除已经无法使用的screen作业。screen -S yourname -&gt; 新建一个叫yourname的sessionscreen -ls         -&gt; 列出当前所有的sessionscreen -r yourname -&gt; 回到yourname这个sessionscreen -d yourname -&gt; 远程detach某个sessionscreen -d -r yourname -&gt; 结束当前session并回到yourname这个session\n常用快捷键C-a ? -&gt; 显示所有键绑定信息C-a c -&gt; 创建一个新的运行shell的窗口并切换到该窗口C-a n -&gt; Next，切换到下一个 window C-a p -&gt; Previous，切换到前一个 window C-a 0..9 -&gt; 切换到第 0..9 个 windowCtrl+a [Space] -&gt; 由视窗0循序切换到视窗9C-a C-a -&gt; 在两个最近使用的 window 间切换 C-a x -&gt; 锁住当前的 window，需用用户密码解锁C-a d -&gt; detach，暂时离开当前session，将目前的 screen session (可能含有多个 windows) 丢到后台执行，并会回到还没进 screen 时的状态，此时在 screen session 里，每个 window 内运行的 process (无论是前台/后台)都在继续执行，即使 logout 也不影响。 C-a z -&gt; 把当前session放到后台执行，用 shell 的 fg 命令则可回去。C-a w -&gt; 显示所有窗口列表C-a t -&gt; time，显示当前时间，和系统的 load C-a k -&gt; kill window，强行关闭当前的 windowC-a [ -&gt; 进入 copy mode，在 copy mode 下可以回滚、搜索、复制就像用使用 vi 一样    C-b Backward，PageUp     C-f Forward，PageDown     H(大写) High，将光标移至左上角     L Low，将光标移至左下角     0 移到行首     $ 行末     w forward one word，以字为单位往前移     b backward one word，以字为单位往后移     Space 第一次按为标记区起点，第二次按为终点     Esc 结束 copy mode C-a ] -&gt; paste，把刚刚在 copy mode 选定的内容贴上\n","categories":["linux"]},{"title":"使用kubekey快速安装k8s","url":"/2025/04/27/%E4%BD%BF%E7%94%A8kubekey%E5%BF%AB%E9%80%9F%E5%AE%89%E8%A3%85k8s/","content":"官方地址https://github.com/kubesphere/kubekey\n安装\ncurl -sfL https://get-kk.kubesphere.io | sh -\n\n单节点测试使用kk create cluster#默认 v1.23.17--with-kubernetes v1.24.1 #默认docker--container-manager containerd#如果不使用--with-kubesphere默认不安装；默认版本为 v3.4.1--with-kubesphere\n多节点kk create config -f deploy.yml#-f 指定配置文件开始安装kk create cluster -f deploy.yml#deploy.yml;其他节点的ip用户名密码的修改成实际的apiVersion: kubekey.kubesphere.io/v1alpha2kind: Clustermetadata:  name: samplespec:  hosts:  - &#123;name: node1, address: 172.16.0.2, internalAddress: 172.16.0.2, user: ubuntu, password: &quot;Qcloud@123&quot;&#125;  - &#123;name: node2, address: 172.16.0.3, internalAddress: 172.16.0.3, user: ubuntu, password: &quot;Qcloud@123&quot;&#125;  roleGroups:    etcd:    - node1    control-plane:     - node1    worker:    - node1    - node2  controlPlaneEndpoint:    ## Internal loadbalancer for apiservers     # internalLoadbalancer: haproxy    domain: lb.kubesphere.local    address: &quot;&quot;    port: 6443  kubernetes:    version: v1.23.17    clusterName: cluster.local    autoRenewCerts: true    containerManager: docker  etcd:    type: kubekey  network:    plugin: calico    kubePodsCIDR: 10.233.64.0/18    kubeServiceCIDR: 10.233.0.0/18    ## multus support. https://github.com/k8snetworkplumbingwg/multus-cni    multusCNI:      enabled: false  registry:    privateRegistry: &quot;&quot;    namespaceOverride: &quot;&quot;    registryMirrors: []    insecureRegistries: []  addons: []----------------------------------------------------#默认不安装kubesphere需要指定--with-kubespherekk create config --with-kubesphere -f deploy-with.yml\n新增删除#新增节点接入集群kk add nodes -f  deploy.yml#删除节点kk delete node &lt;nodeName&gt; -f deploy.yml#删除集群kk delete cluster [-f deploy.yml]\n\n升级集群使用指定版本升级集群。kk upgrade [--with-kubernetes version] [--with-kubesphere version] 仅支持升级 Kubernetes。仅支持升级 KubeSphere。支持升级 Kubernetes 和 KubeSphere。多节点使用指定的配置文件升级集群。kk upgrade [--with-kubernetes version] [--with-kubesphere version] [(-f | --filename) path]如果指定了--with-kubernetes或--with-kubesphere，配置文件也将被更新。用于-f指定为集群创建而生成的配置文件。\n\n更新集群证书\n#默认一年kk  certs renew\n\n","categories":["k8s"]}]